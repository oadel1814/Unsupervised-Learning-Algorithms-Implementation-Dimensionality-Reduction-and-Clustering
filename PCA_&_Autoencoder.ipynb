{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsevyzyL3uAl"
   },
   "source": [
    "## PCA & Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JRLAash5whZ"
   },
   "source": [
    "### import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZLRO0sGP52E-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVxg-CId5-ZF"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z82cQyWC6Aqt"
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "column_names = ['ID', 'Diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "X = df.iloc[:, 2:].values\n",
    "y = df['Diagnosis'].values\n",
    "\n",
    "y = np.where(y == 'M', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai3T-86b6wid"
   },
   "source": [
    "#### checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u3u0BnE6gS6",
    "outputId": "85156c77-41e1-4b86-999a-d8a93f8a97c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwJNmll364Hc"
   },
   "source": [
    "#### Standardizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U-HATfZn6-VV"
   },
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "\n",
    "  mean = np.mean(X, axis=0)\n",
    "  std = np.std(X, axis=0)\n",
    "\n",
    "  std[std == 0] = 1.0\n",
    "  X_std = (X - mean) / std\n",
    "\n",
    "  return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4oeVwc977MvA"
   },
   "outputs": [],
   "source": [
    "X = standardize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4x5p4ME77YI5",
    "outputId": "5da8bc81-b4d6-4208-f2e8-2c8ab47a3c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (569, 30)\n",
      "mean:  -6.826538293184326e-17\n",
      "std:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape: \", X.shape)\n",
    "print(\"mean: \", np.mean(X))\n",
    "print(\"std: \", np.std(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFe8MUd48-GY"
   },
   "source": [
    "## 1. PCA implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OpfpGXSx9CiC"
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "  def __init__(self, n_components):\n",
    "    self.n_components = n_components\n",
    "    self.components = None # Principal components\n",
    "    self.mean = None\n",
    "    self.explained_variance = None # Eigenvalues\n",
    "    self.explained_variance_ratio = None # Ratio of variance explained\n",
    "\n",
    "\n",
    "  def fit(self, X):\n",
    "\n",
    "    # 1. Calculate the mean of the data (needed for centering)\n",
    "    self.mean = np.mean(X, axis=0)\n",
    "\n",
    "    # 2. Center the data (subtract the mean)\n",
    "    X_centered = X - self.mean\n",
    "\n",
    "    # 3. Compute the covariance matrix\n",
    "    cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "    # 4. Perform Eigenvalue Decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # 5. Sort Eigenvalues and Eigenvectors in Descending Order\n",
    "    # eigh returns them in ascending order, so we reverse them\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "    self.explained_variance = eigenvalues[sorted_indices]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # 6. Store the first 'n_components' eigenvectors\n",
    "    # These are the directions of maximum variance (Principal Components)\n",
    "    self.components = sorted_eigenvectors[:, :self.n_components]\n",
    "\n",
    "    # 7. Calculate Explained Variance Ratio\n",
    "\n",
    "    total_variance = np.sum(self.explained_variance)\n",
    "    self.explained_variance_ratio = (\n",
    "        self.explained_variance[:self.n_components] / total_variance\n",
    "    )\n",
    "\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    if self.components is None:\n",
    "      raise Exception(\"PCA must be fit before transform\")\n",
    "\n",
    "    X_centered = X - self.mean\n",
    "    return np.dot(X_centered, self.components)\n",
    "\n",
    "  def inverse_transform(self, X):\n",
    "    return np.dot(X, self.components.T) + self.mean\n",
    "\n",
    "  def get_reconstruction_error(self, X):\n",
    "    X_transformed = self.transform(X)\n",
    "    X_reconstructed = self.inverse_transform(X_transformed)\n",
    "\n",
    "    mse = np.mean(np.sum((X - X_reconstructed) ** 2, axis=1))\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZCk6iwhCK8W"
   },
   "source": [
    "### PCA Class Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIlTCuSf_a8G",
    "outputId": "efec5cb1-61e0-4990-c795-8db6da3d3d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (569, 30)\n",
      "Reduced Data Shape:  (569, 2)\n",
      "Explained Variance Ratio: [0.44272026 0.18971182]\n",
      "Reconstruction Error (MSE): 11.027038\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "error = pca.get_reconstruction_error(X)\n",
    "\n",
    "print(f\"Original Data Shape: {X.shape}\")\n",
    "print(f\"Reduced Data Shape:  {X_pca.shape}\")\n",
    "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio}\")\n",
    "print(f\"Reconstruction Error (MSE): {error:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn4qHnKKCZe5"
   },
   "source": [
    "## 2. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tWS4mG4DJpd"
   },
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qcW9YWx2CiAZ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  x = np.clip(x, -500, 500)\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  s = sigmoid(x)\n",
    "  return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "  return np.tanh(x)\n",
    "\n",
    "def tanh_deravative(x):\n",
    "  return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "  return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DqCmamb9Dt2S"
   },
   "outputs": [],
   "source": [
    "from re import L\n",
    "class Autoencoder:\n",
    "  def __init__(self, input_dim, hidden_layers, bottleneck_dim, activation='relu', learning_rate=0.01, lambda_l2=0.01):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.initial_lr = learning_rate\n",
    "    self.lambda_l2 = lambda_l2\n",
    "\n",
    "    if activation == 'relu':\n",
    "      self.act = relu\n",
    "      self.act_derivative = relu_derivative\n",
    "    elif activation == 'sigmoid':\n",
    "      self.act = sigmoid\n",
    "      self.act_derivative = sigmoid_derivative\n",
    "    elif activation == 'tanh':\n",
    "      self.act = tanh\n",
    "      self.act_derivative = tanh_deravative\n",
    "    else:\n",
    "      raise Exception(\"Unsupported activation\")\n",
    "\n",
    "    layer_dims = [input_dim] + hidden_layers + [bottleneck_dim] + hidden_layers[::-1] + [input_dim]\n",
    "    self.weights = []\n",
    "    self.biases = []\n",
    "\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "      n_in = layer_dims[i]\n",
    "      n_out = layer_dims[i + 1]\n",
    "\n",
    "      scale = np.sqrt(2.0 / n_in)\n",
    "\n",
    "      W = np.random.randn(n_in, n_out) * scale\n",
    "      b = np.zeros((1, n_out))\n",
    "\n",
    "      self.weights.append(W)\n",
    "      self.biases.append(b)\n",
    "\n",
    "  def forward(self, X):\n",
    "    self.Z_store = []\n",
    "    self.A_store = [X]\n",
    "\n",
    "    A = X\n",
    "\n",
    "    for i in range(len(self.weights) - 1):\n",
    "      Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "      A = self.act(Z)\n",
    "\n",
    "      self.Z_store.append(Z)\n",
    "      self.A_store.append(A)\n",
    "\n",
    "    last_idx = len(self.weights) - 1\n",
    "    Z_final = np.dot(A, self.weights[last_idx]) + self.biases[last_idx]\n",
    "    A_final = Z_final\n",
    "\n",
    "    self.Z_store.append(Z_final)\n",
    "    self.A_store.append(A_final)\n",
    "\n",
    "    return A_final\n",
    "\n",
    "  def backward(self, X, Y_pred):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    grads_W = []\n",
    "    grads_b = []\n",
    "\n",
    "    dZ = (Y_pred - X) / m\n",
    "\n",
    "\n",
    "    num_layers = len(self.weights)\n",
    "\n",
    "    for i in reversed(range(num_layers)):\n",
    "      A_prev = self.A_store[i]\n",
    "      W = self.weights[i]\n",
    "\n",
    "      dW = np.dot(A_prev.T, dZ) + (self.lambda_l2 * W)\n",
    "\n",
    "      db = np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "      grads_W.insert(0, dW)\n",
    "      grads_b.insert(0, db)\n",
    "\n",
    "\n",
    "      if(i > 0):\n",
    "        dZ = np.dot(dZ, W.T)\n",
    "        dZ = dZ * self.act_derivative(self.Z_store[i - 1])\n",
    "    return grads_W, grads_b\n",
    "\n",
    "\n",
    "  def update_parameters(self, grads_W, grads_b):\n",
    "    for i in range(len(self.weights)):\n",
    "      self.weights[i] -= self.learning_rate * grads_W[i]\n",
    "      self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "\n",
    "  def get_bottleneck(self, X):\n",
    "    bottleneck_layer_idx = (len(self.weights) // 2)\n",
    "\n",
    "    A = X\n",
    "    for i in range(bottleneck_layer_idx):\n",
    "        Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "        A = self.act(Z)\n",
    "\n",
    "    return A\n",
    "\n",
    "  def fit(self, X, epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Training loop with Mini-Batch GD and Learning Rate Scheduling.\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Mini-batch loop\n",
    "            for start_idx in range(0, X.shape[0], batch_size):\n",
    "                end_idx = min(start_idx + batch_size, X.shape[0])\n",
    "                batch_X = X_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # 1. Forward\n",
    "                Y_pred = self.forward(batch_X)\n",
    "\n",
    "                # 2. Compute Loss (MSE + L2)\n",
    "                mse_loss = np.mean((batch_X - Y_pred) ** 2)\n",
    "                l2_loss = 0\n",
    "                for W in self.weights:\n",
    "                    l2_loss += np.sum(W ** 2)\n",
    "\n",
    "                total_loss = mse_loss + (0.5 * self.lambda_l2 * l2_loss)\n",
    "                epoch_loss += total_loss\n",
    "\n",
    "                # 3. Backward\n",
    "                grads_W, grads_b = self.backward(batch_X, Y_pred)\n",
    "\n",
    "                # 4. Update\n",
    "                self.update_parameters(grads_W, grads_b)\n",
    "\n",
    "            # Average loss for the epoch\n",
    "            avg_loss = epoch_loss / (X.shape[0] / batch_size)\n",
    "            loss_history.append(avg_loss)\n",
    "\n",
    "            # Learning Rate Scheduling (Decay)\n",
    "            # Example: Decay by 5% every 10 epochs\n",
    "            if epoch % 10 == 0 and epoch > 0:\n",
    "                self.learning_rate *= 0.95\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBD_nBaPN4tG",
    "outputId": "d6e8ef26-6691-473a-dd4a-0ff406ddd4df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder...\n",
      "Epoch 0/50 - Loss: 2.0776\n",
      "Epoch 10/50 - Loss: 1.4532\n",
      "Epoch 20/50 - Loss: 1.3072\n",
      "Epoch 30/50 - Loss: 1.1765\n",
      "Epoch 40/50 - Loss: 1.1078\n",
      "Encoded Shape: (569, 5)\n",
      "Final Loss: 1.0928\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Architecture\n",
    "# Input: 30 features\n",
    "# Hidden: [20, 10] -> This creates layers: 30->20->10->Bottleneck->10->20->30\n",
    "# Bottleneck: 5\n",
    "ae = Autoencoder(\n",
    "    input_dim=X.shape[1],\n",
    "    hidden_layers=[20, 10],\n",
    "    bottleneck_dim=5,\n",
    "    activation='relu',\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# 2. Train\n",
    "print(\"Training Autoencoder...\")\n",
    "loss_curve = ae.fit(X, epochs=50, batch_size=32)\n",
    "\n",
    "# 3. Get Compressed Data\n",
    "X_encoded = ae.get_bottleneck(X)\n",
    "\n",
    "print(f\"Encoded Shape: {X_encoded.shape}\") # Should be (569, 5)\n",
    "print(f\"Final Loss: {loss_curve[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
